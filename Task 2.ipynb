{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Using pre trained embedding - (fasttext trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/reddit_200k_train.csv\",encoding=\"iso-8859-1\")[[\"body\",\"REMOVED\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/reddit_200k_test.csv\",encoding=\"iso-8859-1\")[[\"body\",\"REMOVED\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>REMOVED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've always been taught it emerged from the ea...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As an ECE, my first feeling as \"HEY THAT'S NOT...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Monday: Drug companies stock dives on good new...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i learned that all hybrids are unfertile i won...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well i was wanting to get wasted tonight.  Not...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  REMOVED\n",
       "0  I've always been taught it emerged from the ea...    False\n",
       "1  As an ECE, my first feeling as \"HEY THAT'S NOT...     True\n",
       "2  Monday: Drug companies stock dives on good new...     True\n",
       "3  i learned that all hybrids are unfertile i won...    False\n",
       "4  Well i was wanting to get wasted tonight.  Not...    False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[\"body\"]\n",
    "y_train = df_train[\"REMOVED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test[\"body\"]\n",
    "y_test = df_test[\"REMOVED\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use pretrained word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "w = models.KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    output =[]\n",
    "    for word in doc:\n",
    "        if word in w.vocab:\n",
    "            output.append(word)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_w2v = CountVectorizer(token_pattern=r\"\\b[^\\d\\W_]+\\b\",min_df=2,stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_w2v.fit(X_train)\n",
    "docs_train = vect_w2v.inverse_transform(vect_w2v.transform(X_train))\n",
    "docs_train = [preprocess(doc) for doc in docs_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test = vect_w2v.inverse_transform(vect_w2v.transform(X_test))\n",
    "docs_test = [preprocess(doc) for doc in docs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(docs_val):\n",
    "    X_train=[]\n",
    "    i=-1\n",
    "    index_array =[]\n",
    "    for doc in docs_val:\n",
    "        i=i+1\n",
    "        sum0 = 0\n",
    "        n=0\n",
    "        if len(doc)>=1:\n",
    "            for d in doc:\n",
    "                sum0 = sum0 + w[d]\n",
    "                n=n+1\n",
    "            avg = sum0/n\n",
    "            X_train.append(avg)\n",
    "        else:\n",
    "            index_array.append(i)\n",
    "    return np.array(X_train),index_array\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb, train_ind_drop = sentence_embedding(docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_emb, test_ind_drop = sentence_embedding(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_emb = y_train.drop(train_ind_drop,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_emb = y_test.drop(test_ind_drop,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: Using Word2Vec to get sentence embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we use a pre-trained word-vectors generated using fasttext.\n",
    "\n",
    "We first import the word vectors trained over Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset.\n",
    "\n",
    "This is a 300-dimensional word vector with 1 million word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score on word2vec model\n",
      "0.7265647985815331\n"
     ]
    }
   ],
   "source": [
    "pipe  =  make_pipeline(LogisticRegression(solver=\"sag\"))\n",
    "print(\"Cross val score on word2vec model\")\n",
    "pipe.fit(X_train_emb,y_train_emb)\n",
    "print(np.mean(cross_val_score(pipe,X_train_emb,y_train_emb,cv=5,scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc-auc score on test set\n",
      "0.6281712173401774\n"
     ]
    }
   ],
   "source": [
    "y_preds = pipe.predict(X_test_emb)\n",
    "print(\"Roc-auc score on test set\")\n",
    "print(roc_auc_score(y_test_emb, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"logisticregression__C\": [100,10,1,0.1,0.01],\n",
    "             }\n",
    "grid = GridSearchCV(make_pipeline(LogisticRegression(solver=\"sag\"),\n",
    "                                  memory=\"cache_folder\"),\n",
    "                    param_grid=param_grid, cv=5, scoring=\"roc_auc\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory='cache_folder',\n",
       "     steps=[('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'logisticregression__C': [100, 10, 1, 0.1, 0.01]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train_emb, y_train_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7265648822809037"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression__C': 1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score on word2vec model after grid search\n",
      "0.7265648279548907\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross val score on word2vec model after grid search\")\n",
    "print(np.mean(cross_val_score(grid,X_train_emb,y_train_emb,cv=5,scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(train_ind_drop,axis=0)\n",
    "df_test = df_test.drop(test_ind_drop,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach using word2vec embeddings is better than our baseline model but it doesnt outperform our approach using n-grams, characters, tf-idf rescaling, stop words, token patterns and infrequent word removal in task 1.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore other features you can derive from the text, such as html, length, punctuation, capitalization in addition to word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of words that are all caps - could indicate spam comments if count is high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCapWordsCount(sentence):\n",
    "    count=0\n",
    "    for word in sentence.split():\n",
    "        if word.isupper() and len(word)>2:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Cap_words_count\"] = df_train.apply(lambda row: getCapWordsCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Cap_words_count\"] = df_test.apply(lambda row: getCapWordsCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of punctuation - Use of too many ! indicate spam comments and ? indicate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPunctuationCount(sentence):\n",
    "    count=0\n",
    "    for word in sentence:\n",
    "        if word in [\"!\",\"?\"]:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Punc_words_count\"] = df_train.apply(lambda row: getPunctuationCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Punc_words_count\"] = df_test.apply(lambda row: getPunctuationCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Length - Very short or very long sentences might be spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceLength(sentence):\n",
    "    return len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Sentence_length\"] = df_train.apply(lambda row: getSentenceLength(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Sentence_length\"] = df_test.apply(lambda row: getSentenceLength(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word count in sentence - very few words or too many words might be spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsCount(sentence):\n",
    "    count=0\n",
    "    for word in sentence.split():\n",
    "        count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Word_Count\"] = df_train.apply(lambda row: getWordsCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Word_Count\"] = df_test.apply(lambda row: getWordsCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNounsCount(sentence):\n",
    "    sentence_nouns = []\n",
    "    is_noun = lambda pos: pos == 'NOUN'\n",
    "    sentence = nltk.sent_tokenize(sentence)\n",
    "    sentence = [nltk.word_tokenize(sent) for sent in sentence]\n",
    "    for sent in sentence:\n",
    "        sentence_nouns.append([word for (word, pos) in nltk.pos_tag(sent,tagset='universal') if is_noun(pos)])\n",
    "    return len(sentence_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Noun_Count\"] = df_train.apply(lambda row: getNounsCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Noun_Count\"] = df_test.apply(lambda row: getNounsCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdjCount(sentence):\n",
    "    sentence_nouns = []\n",
    "    is_noun = lambda pos: pos == 'ADJ'\n",
    "    sentence = nltk.sent_tokenize(sentence)\n",
    "    sentence = [nltk.word_tokenize(sent) for sent in sentence]\n",
    "    for sent in sentence:\n",
    "        sentence_nouns.append([word for (word, pos) in nltk.pos_tag(sent,tagset='universal') if is_noun(pos)])\n",
    "    return len(sentence_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Adj_Count\"] = df_train.apply(lambda row: getAdjCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Adj_Count\"] = df_test.apply(lambda row: getAdjCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPronounCount(sentence):\n",
    "    sentence_nouns = []\n",
    "    is_noun = lambda pos: pos == 'PRON'\n",
    "    sentence = nltk.sent_tokenize(sentence)\n",
    "    sentence = [nltk.word_tokenize(sent) for sent in sentence]\n",
    "    for sent in sentence:\n",
    "        sentence_nouns.append([word for (word, pos) in nltk.pos_tag(sent,tagset='universal') if is_noun(pos)])\n",
    "    return len(sentence_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Pronoun_Count\"] = df_train.apply(lambda row: getPronounCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Pronoun_Count\"] = df_test.apply(lambda row: getPronounCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVerbCount(sentence):\n",
    "    sentence_nouns = []\n",
    "    is_noun = lambda pos: pos == 'VERB'\n",
    "    sentence = nltk.sent_tokenize(sentence)\n",
    "    sentence = [nltk.word_tokenize(sent) for sent in sentence]\n",
    "    for sent in sentence:\n",
    "        sentence_nouns.append([word for (word, pos) in nltk.pos_tag(sent,tagset='universal') if is_noun(pos)])\n",
    "    return len(sentence_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Verb_Count\"] = df_train.apply(lambda row: getVerbCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Verb_Count\"] = df_test.apply(lambda row: getVerbCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link present or absent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_link(data):\n",
    "    if \"http\" in data:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Link'] = df_train.apply(lambda row: contains_link(row['body']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Link'] = df_test.apply(lambda row: contains_link(row['body']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative sentiment - might indicate harsh language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_neg(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    return score['neg']\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Negative_sent\"] = df_train.apply(lambda row: sentiment_analyzer_neg(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Negative_sent\"] = df_test.apply(lambda row: sentiment_analyzer_neg(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_pos(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    return score['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Positive_sent\"] = df_train.apply(lambda row: sentiment_analyzer_pos(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Positive_sent\"] = df_test.apply(lambda row: sentiment_analyzer_pos(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression using engineered features and body feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop([\"body\",\"REMOVED\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.drop([\"body\",\"REMOVED\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= np.hstack((X_train_emb,np.array(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test= np.hstack((X_test_emb,np.array(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver=\"sag\")\n",
    "lr.fit(X_train,y_train_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc-auc score on test set\n",
      "0.5011016674916332\n"
     ]
    }
   ],
   "source": [
    "y_preds = lr.predict(X_test)\n",
    "print(\"Roc-auc score on test set\")\n",
    "print(roc_auc_score(y_test_emb, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"logisticregression__C\": [100,10,1,0.1,0.01],\n",
    "             }\n",
    "grid = GridSearchCV(make_pipeline(LogisticRegression(solver=\"sag\"),\n",
    "                                  memory=\"cache_folder\"),\n",
    "                    param_grid=param_grid, cv=5, scoring=\"roc_auc\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory='cache_folder',\n",
       "     steps=[('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'logisticregression__C': [100, 10, 1, 0.1, 0.01]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6647347712696008"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression__C': 100}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score after grid search\n",
      "0.6647319497134462\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross val score after grid search\")\n",
    "print(np.mean(cross_val_score(grid,X_train,y_train_emb,cv=5,scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding our engineered features to the best model we got in task 1.2  in addition to word2vec did not improve performance. This indicates that there might not be a pattern related to capitalization, punctuation, links, pos tagging and sentiment analysis that differentiates comments that have been removed from ones that havent been removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of task 2, the best model we have is using using n-grams, characters, tf-idf rescaling, stop words, token patterns and infrequent word removal with no feature engineering. This gives an roc-auc score of 0.76 (Task 1.2 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
