{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/reddit_200k_train.csv\",encoding=\"iso-8859-1\")[[\"body\",\"REMOVED\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/reddit_200k_test.csv\",encoding=\"iso-8859-1\")[[\"body\",\"REMOVED\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>REMOVED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've always been taught it emerged from the ea...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As an ECE, my first feeling as \"HEY THAT'S NOT...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Monday: Drug companies stock dives on good new...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i learned that all hybrids are unfertile i won...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well i was wanting to get wasted tonight.  Not...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  REMOVED\n",
       "0  I've always been taught it emerged from the ea...    False\n",
       "1  As an ECE, my first feeling as \"HEY THAT'S NOT...     True\n",
       "2  Monday: Drug companies stock dives on good new...     True\n",
       "3  i learned that all hybrids are unfertile i won...    False\n",
       "4  Well i was wanting to get wasted tonight.  Not...    False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from gensim import corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[\"body\"]\n",
    "y_train = df_train[\"REMOVED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test[\"body\"]\n",
    "y_test = df_test[\"REMOVED\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Baseline model - baseline model using a bag-of-words approach and a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a bag of words approach using CountVectorizer with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score on baseline model\n",
      "0.7188717147032742\n"
     ]
    }
   ],
   "source": [
    "pipe  =  make_pipeline(CountVectorizer(),LogisticRegression(solver=\"sag\"))\n",
    "print(\"Cross val score on baseline model\")\n",
    "pipe.fit(X_train,y_train)\n",
    "print(np.mean(cross_val_score(pipe,X_train,y_train,cv=5,scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc-auc score on test set\n",
      "0.5820846015686182\n"
     ]
    }
   ],
   "source": [
    "y_preds = pipe.predict(X_test)\n",
    "print(\"Roc-auc score on test set\")\n",
    "print(roc_auc_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"logisticregression__C\": [100,10,1,0.1,0.01],\n",
    "             }\n",
    "grid = GridSearchCV(make_pipeline(CountVectorizer(),LogisticRegression(solver=\"sag\"),\n",
    "                                  memory=\"cache_folder\"),\n",
    "                    param_grid=param_grid, cv=5, scoring=\"roc_auc\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory='cache_folder',\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "  ... penalty='l2', random_state=None, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'logisticregression__C': [100, 10, 1, 0.1, 0.01]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7188801150880733"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression__C': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score on baseline model after grid search\n",
      "0.7188831216291589\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross val score on baseline model after grid search\")\n",
    "print(np.mean(cross_val_score(grid,X_train,y_train,cv=5,scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Try using n-grams, characters, tf-idf rescaling and possibly other ways to tune the BoW model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using infrequent word removal, stop words and token patterns to restrict the vocaublary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TF-IDF Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use token pattern parameter to restrict the kind of acceptable tokens - only letters, no digits, no underscores. We removed the stopwords. We use min_df parameter to only consider tokens that occur atleast 2 times in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf rescaling is used to down-weight tokens that are very common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score using tf-idf transformer\n",
      "0.767313738740652\n"
     ]
    }
   ],
   "source": [
    "pipe  =  make_pipeline(CountVectorizer(token_pattern=r\"\\b[^\\d\\W_]+\\b\",min_df=2,stop_words=\"english\"),TfidfTransformer(),LogisticRegression(solver=\"sag\"))\n",
    "print(\"Cross val score using tf-idf transformer\")\n",
    "print(np.mean(cross_val_score(pipe,X_train,y_train,cv=5,scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TF-IDF transformer improves performance as compared to baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downweighting very common words improves the performance for this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams is used to look at pairs of words that appear next to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at only unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngram_range=(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score using n-grams\n",
      "0.6672887809116439\n"
     ]
    }
   ],
   "source": [
    "pipe  =  make_pipeline(CountVectorizer(token_pattern=r\"\\b[^\\d\\W_]+\\b\",min_df=2,ngram_range=(1,1),stop_words=\"english\"),LogisticRegression(solver=\"sag\"))\n",
    "print(\"Cross val score using n-grams\")\n",
    "print(np.mean(cross_val_score(pipe,X_train,y_train,cv=5,scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ngrams with range (1,1) does not improve performance as compared to baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngram_range=(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at unigrams and bigrams. This gives more context as compared to unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score using n-grams\n",
      "0.660401562493334\n"
     ]
    }
   ],
   "source": [
    "pipe  =  make_pipeline(CountVectorizer(token_pattern=r\"\\b[^\\d\\W_]+\\b\",min_df=2,ngram_range=(1,2),stop_words=\"english\"),LogisticRegression(solver=\"sag\"))\n",
    "print(\"Cross val score using n-grams\")\n",
    "print(np.mean(cross_val_score(pipe,X_train,y_train,cv=5,scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ngrams with range (1,2) does not improve performance as compared to baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more context(bigrams) as compared to unigrams worsens performance a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at only bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngram_range=(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score using n-grams\n",
      "0.6452343885887948\n"
     ]
    }
   ],
   "source": [
    "pipe  =  make_pipeline(CountVectorizer(token_pattern=r\"\\b[^\\d\\W_]+\\b\",min_df=2,ngram_range=(2,2),stop_words=\"english\"),LogisticRegression(solver=\"sag\"))\n",
    "print(\"Cross val score using n-grams\")\n",
    "print(np.mean(cross_val_score(pipe,X_train,y_train,cv=5,scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ngrams with range (2,2) does not improve performance as compared to baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs worse than looking at unigrams only and looking at unigrams and bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to add more context in the form on trigrams. We look at unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngram_range=(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score using n-grams\n",
      "0.6571242886465027\n"
     ]
    }
   ],
   "source": [
    "pipe  =  make_pipeline(CountVectorizer(token_pattern=r\"\\b[^\\d\\W_]+\\b\",min_df=2,ngram_range=(1,3),stop_words=\"english\"),LogisticRegression(solver=\"sag\"))\n",
    "print(\"Cross val score using n-grams\")\n",
    "print(np.mean(cross_val_score(pipe,X_train,y_train,cv=5,scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ngrams with range (1,3) does not improve performance as compared to baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is worse as compared to unigrams only and unigrams and bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram range (1,1) gives the best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more context doesnt help in this particular problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using character n-grams - can be helpful to be more robust towards misspelling or obfuscation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using word boundary - respects word boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score using character analyser\n",
      "0.6917584429147609\n"
     ]
    }
   ],
   "source": [
    "pipe  =  make_pipeline(CountVectorizer(token_pattern=r\"\\b[^\\d\\W_]+\\b\",min_df=2,stop_words=\"english\",analyzer=\"char_wb\"),LogisticRegression(solver=\"sag\"))\n",
    "print(\"Cross val score using character analyser\")\n",
    "print(np.mean(cross_val_score(pipe,X_train,y_train,cv=5,scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using character analyzer does not improve performance as compared to baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive -  does not respect character boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score using character analyser\n",
      "0.6960119397846068\n"
     ]
    }
   ],
   "source": [
    "pipe  =  make_pipeline(CountVectorizer(token_pattern=r\"\\b[^\\d\\W_]+\\b\",min_df=2,stop_words=\"english\",analyzer=\"char\"),LogisticRegression(solver=\"sag\"))\n",
    "print(\"Cross val score using character analyser\")\n",
    "print(np.mean(cross_val_score(pipe,X_train,y_train,cv=5,scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using character analyzer does not improve performance as compared to baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyzer=\"char\" gives better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combining tf-idf transformer, count vectorizer, character analyzer -  we use best parameter values obtained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score using tf-idf transformer, count vectorizer, character analyzer\n",
      "0.7683041454603811\n"
     ]
    }
   ],
   "source": [
    "pipe  =  make_pipeline(CountVectorizer(token_pattern=r\"\\b[^\\d\\W_]+\\b\",min_df=2,stop_words=\"english\",analyzer=\"char\",ngram_range=(2,2)),TfidfTransformer(),LogisticRegression(solver=\"sag\"))\n",
    "print(\"Cross val score using tf-idf transformer, count vectorizer, character analyzer\")\n",
    "print(np.mean(cross_val_score(pipe,X_train,y_train,cv=5,scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance improves significantly as compared to baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc-auc score on test set\n",
      "0.6628629184710754\n"
     ]
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "y_preds = pipe.predict(X_test)\n",
    "print(\"Roc-auc score on test set\")\n",
    "print(roc_auc_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance on test set also improves as compared to baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"logisticregression__C\": [100,10,1,0.1,0.01],\n",
    "             }\n",
    "grid = GridSearchCV(make_pipeline(CountVectorizer(token_pattern=r\"\\b[^\\d\\W_]+\\b\",min_df=2,stop_words=\"english\",analyzer=\"char\",ngram_range=(2,2)),TfidfTransformer(),LogisticRegression(solver=\"sag\"),\n",
    "                                  memory=\"cache_folder\"),\n",
    "                    param_grid=param_grid, cv=5, scoring=\"roc_auc\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory='cache_folder',\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(2, 2), preprocessor=None, stop_words='english... penalty='l2', random_state=None, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'logisticregression__C': [100, 10, 1, 0.1, 0.01]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7683306035804637"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression__C': 10}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score after grid search\n",
      "0.7683044550126812\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross val score after grid search\")\n",
    "print(np.mean(cross_val_score(grid,X_train,y_train,cv=5,scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence an approach using n-grams, characters, tf-idf rescaling, stop words, token patterns and infrequent word removal is better than our baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Explore other features you can derive from the text, such as html, length, punctuation, capitalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of words that are all caps - could indicate spam comments if count is high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCapWordsCount(sentence):\n",
    "    count=0\n",
    "    for word in sentence.split():\n",
    "        if word.isupper() and len(word)>2:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Cap_words_count\"] = df_train.apply(lambda row: getCapWordsCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Cap_words_count\"] = df_test.apply(lambda row: getCapWordsCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of punctuation - Use of too many ! indicate spam comments and ? indicate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPunctuationCount(sentence):\n",
    "    count=0\n",
    "    for word in sentence:\n",
    "        if word in [\"!\",\"?\"]:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Punc_words_count\"] = df_train.apply(lambda row: getPunctuationCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Punc_words_count\"] = df_test.apply(lambda row: getPunctuationCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Length - Very short or very long sentences might be spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceLength(sentence):\n",
    "    return len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Sentence_length\"] = df_train.apply(lambda row: getSentenceLength(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Sentence_length\"] = df_test.apply(lambda row: getSentenceLength(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word count in sentence - very few words or too many words might be spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsCount(sentence):\n",
    "    count=0\n",
    "    for word in sentence.split():\n",
    "        count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Word_Count\"] = df_train.apply(lambda row: getWordsCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Word_Count\"] = df_test.apply(lambda row: getWordsCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNounsCount(sentence):\n",
    "    sentence_nouns = []\n",
    "    is_noun = lambda pos: pos == 'NOUN'\n",
    "    sentence = nltk.sent_tokenize(sentence)\n",
    "    sentence = [nltk.word_tokenize(sent) for sent in sentence]\n",
    "    for sent in sentence:\n",
    "        sentence_nouns.append([word for (word, pos) in nltk.pos_tag(sent,tagset='universal') if is_noun(pos)])\n",
    "    return len(sentence_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Noun_Count\"] = df_train.apply(lambda row: getNounsCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Noun_Count\"] = df_test.apply(lambda row: getNounsCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/ankitpeshin/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('universal_tagset')\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "def getAdjCount(sentence):\n",
    "    sentence_nouns = []\n",
    "    is_noun = lambda pos: pos == 'ADJ'\n",
    "    sentence = nltk.sent_tokenize(sentence)\n",
    "    sentence = [nltk.word_tokenize(sent) for sent in sentence]\n",
    "    for sent in sentence:\n",
    "        sentence_nouns.append([word for (word, pos) in nltk.pos_tag(sent,tagset='universal') if is_noun(pos)])\n",
    "    return len(sentence_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Adj_Count\"] = df_train.apply(lambda row: getAdjCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Adj_Count\"] = df_test.apply(lambda row: getAdjCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPronounCount(sentence):\n",
    "    sentence_nouns = []\n",
    "    is_noun = lambda pos: pos == 'PRON'\n",
    "    sentence = nltk.sent_tokenize(sentence)\n",
    "    sentence = [nltk.word_tokenize(sent) for sent in sentence]\n",
    "    for sent in sentence:\n",
    "        sentence_nouns.append([word for (word, pos) in nltk.pos_tag(sent,tagset='universal') if is_noun(pos)])\n",
    "    return len(sentence_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Pronoun_Count\"] = df_train.apply(lambda row: getPronounCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Pronoun_Count\"] = df_test.apply(lambda row: getPronounCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVerbCount(sentence):\n",
    "    sentence_nouns = []\n",
    "    is_noun = lambda pos: pos == 'VERB'\n",
    "    sentence = nltk.sent_tokenize(sentence)\n",
    "    sentence = [nltk.word_tokenize(sent) for sent in sentence]\n",
    "    for sent in sentence:\n",
    "        sentence_nouns.append([word for (word, pos) in nltk.pos_tag(sent,tagset='universal') if is_noun(pos)])\n",
    "    return len(sentence_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Verb_Count\"] = df_train.apply(lambda row: getVerbCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Verb_Count\"] = df_test.apply(lambda row: getVerbCount(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link present or absent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_link(data):\n",
    "    if \"http\" in data:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Link'] = df_train.apply(lambda row: contains_link(row['body']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Link'] = df_test.apply(lambda row: contains_link(row['body']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative sentiment - might indicate harsh language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_neg(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    return score['neg']\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Negative_sent\"] = df_train.apply(lambda row: sentiment_analyzer_neg(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Negative_sent\"] = df_test.apply(lambda row: sentiment_analyzer_neg(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_pos(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    return score['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Positive_sent\"] = df_train.apply(lambda row: sentiment_analyzer_pos(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Positive_sent\"] = df_test.apply(lambda row: sentiment_analyzer_pos(row[\"body\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression using only engineered features ie. no body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop([\"body\",\"REMOVED\"],axis=1)\n",
    "y_train = df_train[\"REMOVED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score using engineered features only\n",
      "0.6644582068447398\n"
     ]
    }
   ],
   "source": [
    "pipe  =  make_pipeline(LogisticRegression(solver=\"sag\"))\n",
    "print(\"Cross val score using engineered features only\")\n",
    "print(np.mean(cross_val_score(pipe,X_train,y_train,cv=5,scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is not better than baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression using engineered features and body feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop([\"body\",\"REMOVED\"],axis=1)\n",
    "X_train_body = df_train[\"body\"]\n",
    "y_train = df_train[\"REMOVED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.drop([\"body\",\"REMOVED\"],axis=1)\n",
    "X_test_body = df_test[\"body\"]\n",
    "y_test = df_test[\"REMOVED\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " using n-grams, characters, tf-idf rescaling, stop words, token patterns and infrequent word removal approach that gave best performance in task 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe  =  make_pipeline(CountVectorizer(token_pattern=r\"\\b[^\\d\\W_]+\\b\",min_df=2,ngram_range=(1,1),stop_words=\"english\",analyzer=\"char\"), TfidfTransformer())\n",
    "X_train_body_vectorized = pipe.fit_transform(X_train_body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_body_vectorized = pipe.transform(X_test_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= hstack((X_train_body_vectorized,np.array(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test= hstack((X_test_body_vectorized,np.array(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver=\"sag\")\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc-auc score on test set\n",
      "0.5013500849866808\n"
     ]
    }
   ],
   "source": [
    "y_preds = lr.predict(X_test)\n",
    "print(\"Roc-auc score on test set\")\n",
    "print(roc_auc_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding engineered features gives similar results to baseline model on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"logisticregression__C\": [100,10,1,0.1,0.01],\n",
    "             }\n",
    "grid = GridSearchCV(make_pipeline(LogisticRegression(solver=\"sag\"),\n",
    "                                  memory=\"cache_folder\"),\n",
    "                    param_grid=param_grid, cv=5, scoring=\"roc_auc\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory='cache_folder',\n",
       "     steps=[('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'logisticregression__C': [100, 10, 1, 0.1, 0.01]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6645328052052749"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression__C': 100}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score after grid search\n",
      "0.6645310851304156\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross val score after grid search\")\n",
    "print(np.mean(cross_val_score(grid,X_train,y_train,cv=5,scoring=\"roc_auc\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding our engineered features to the best model we got in task 1.2  using n-grams, characters, tf-idf rescaling, stop words, token patterns and infrequent word removal did not improve performance. This indicates that there might not be a pattern related to capitalization, punctuation, links, pos tagging and sentiment analysis that differentiates comments that have been removed from ones that havent been removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of task 1, the best model we have is using using n-grams, characters, tf-idf rescaling, stop words, token patterns and infrequent word removal with no feature engineering. This gives an roc-auc score of 0.76"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
